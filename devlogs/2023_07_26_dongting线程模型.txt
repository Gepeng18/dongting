今天的文章很简单，介绍一下dongting的线程模型，现在项目还没什么文档，我会用文章介绍一些基本的项目概念，有助于项目的阅读。
总的来说，为了减少多线程竞争的开销，dongting很多地方是单线程模式。这不是说只有一个线程，而是有多个线程各司其职，每个线程都有其负责的范围，在dongting程序开发中一个重要的事情就是搞清楚数据是哪个线程负责，线程读写自己的数据不需要任何额外的保护。但也有一些边界数据需要线程之间协作，线程之间通常通过内存消息队列来通信。
理论上，是可以只用一个线程的，就像某些线程语言或者产品那样，这样搞就无法利用多核心，需要启动多个进程才能充分利用CPU，这通常不太适合Java程序，我没有选择这样做。
先讲讲dongting的rpc模块，有NioClient和NioServer两个类。
NioServer是个1+N的线程模型。其中1个线程负责处理socket接入的请求，线程名是${name}IoAccept，连接建立后根据hash值交给worker线程。worker线程有N个，线程名是${name}IoWorker${index}，这个数字可以设置，默认值是根据CPU的核心数计算出来的，如果核心数很多，自动计算出来的N要远小于核心数。很多程序会自动设置为CPU核心数，甚至更多，但我认为，IO线程不应该有那么多，毕竟业务程序也是要干活的，线程多反而效率更低。
在NioServer这里，每个socket连接建立以后，就和一个worker线程建立了绑定，它的读写以后都由这个worker线程负责，数据包的编解码的工作通常也在worker线程中进行。每个worker会负责多个socket。在某些情况下，这可能导致不同worker的负载不均匀，我原来想过要不要处理这个问题，但因为过于复杂，暂时先作罢了。
同样是出于简单化的原因，虽然NioClient可以连接多个server（多个socket），但也只有一个线程，线程名是${name}IoWorker。它和NioServer用的其实是同一个类NioWorker，这是因为，连接建立以后，两边都是对等的，大部分情况下不需要区分server和client，这里也就尽量复用了。
用户线程要发起rpc请求时，NioClient将用户请求数据写入一个MPSC queue，每个worker会负责一个queue，它不断从queue中轮询，得到数据后就执行写入或其它处理。
接下来讲一下raft模块，dongting实现了multi-raft，可以有多个raft group同时运行，不仅raft group内的成员可以动态增减，raft group本身也是可以动态增删的。raft模块的线程模型也是1+N，有多少个raft group，就有多少个raft线程，再加一个schedule线程。
每个raft线程只负责自己的数据，比如用来保存状态数据的RaftStatus类，它的每个实例都由对应的raft线程负责，但是部分状态数据可能需要在其它线程读。为了让其它线程读取时看到的状态是一致的，RaftStatusImpl的copyShareStatus()将这些共享字段原子的复制到一个只读对象中，其它线程只从这个只读对象读取。
由于运行raft的node和raft group会动态增删，node信息、group信息这些元数据是全局的，显然不能由某个raft线程独享（维护），所以，dongting提供了一个schedule线程来做这些事情，当涉及到这些全局数据的修改时，通过queue将任务提交给schedule线程。比如NodeManager的字段就归schedule线程所有，默认要求运行在schedule线程中。
大部分时候，每个线程读写其专有数据是很简单的，但个别情况也会很扭曲，比如变更raft成员的prepare阶段的执行（ApplyManager的doPrepare方法），本来在raft线程中执行，为了做点检查以及更新一下node的引用计数，不得不写queue将执行权交给schedule线程，schedule线程干完这点事又需要将执行权交还给raft线程。
除了raft线程和schedule线程以外，默认的raft log实现还有一个IO线程池，它可以跨raft group共享，主要用来执行AsynchronousFileChannel的异步IO任务。
由于dongting是异步操作网络和存储的，所以还有一个“并发”问题需要注意，虽然某些数据归单个线程负责，不会有线程竞争，但异步IO发生时，线程会继续执行其它任务，有可能也会更新之前暂停的（等待IO结果）任务的数据，所以异步IO返回的时候，之前检查过的一些状态，根据情况可能需要再检查一遍。
以上就是今天的这篇文章了，下一篇文章我要谈一下dongting项目的定位和原则。