对不起，我来晚了。

我要开发一个新的中间件引擎，目标是0依赖，10倍性能，1/100大小，100倍启动速度。
这个目标没有变，这个公众号也没有废弃，最近几个月我每天都在干这件事，现在是时候报告一下阶段性的进展了。

## 项目的一些背景

先讲一下这个项目的背景，有很多原因，这里简单说几个。

我在快手负责了将近3年RocketMQ，发现部署在物理服务器上的时候，压测不能充分跑满硬件。
当然对于一个复杂软件，充分利用硬件是不容易的，涉及到硬件、操作系统、应用软件等等。
如果不能理解，可以想象一个大公司，CEO野心勃勃的PUSH全公司硬核突击，很多人因此累的半死，然而总是还有很多人在舒服的摸鱼（我不是专指twitter😂），复杂场景充分利用硬件的潜力就和大公司里充分利用人（团队）的潜力一样是很难的。

当时做了很多测试和优化，在GitHub也提了一些PR，主要分两批集成进4.9.1和4.9.3版本，有一些提升，但不能充分利用硬件能力的情况仍然存在。
我现在能认识到其中一些原因，肯定仍还有一些原因在我认知之外。

除此之外，我对性能数据也不满意。提升硬件使用率是过程，性能数据是结果，光吭哧干活不出结果也不行，对吧？
有人跟我说测到过很大的数字，但这对我没意义。我需要的结果数字第一可以重复验证，第二是普适。
重复验证就像测试的复现，任何人用相同的软硬件条件就可以重新测出这个结果，否则你说多少就是多少啊？
普适就是普遍适用，尽量不指定特定的场景、或者特定的配置参数（比如很多场景下应用部署者都没有权限调整内核参数），也就是要追求内在的本质性能提升。
举个例子，有个程序性能很差，疯狂gc，通过调整VM参数，吞吐提升了50%，这是治标，而治本需要讨论为什么会疯狂gc，是不是哪里写的很烂。
治标只应该用于应急，基础软件更应该关注治本。

选择做一个mq/raft/config/rpc四合一的东西，是因为我觉得它们之间有紧密的联系。关于这一点这里不展开了。

还有一点，现在Java领域的jar包依赖有爆炸的趋势，这个趋势还没有得到足够的重视。
不管是公司内的二方包还是开源的三方包，你随便引入一个东西，传递的依赖都很多，项目复杂了，打个包都1G以上。包大了那就发布慢，启动慢。
以前还在厂里面上班的时候，看着别人团队轰轰烈烈的搞依赖治理，那画面，啧啧啧。。。
对这个问题我也有一些思考，这当然和Java动态连接，动态加载的性质有关，以后有机会我专门撰文说明。
更重要的是需要架构师管理好依赖，不要随便引入依赖，把问题消灭在前期而不是后期再治理，可能有人觉得这做不到，那我来做个表率，0依赖开发一个中间件，如何？
很多类库、框架、中间件也到了该重新发明的时候了。

最后，类似的东西在大厂里面都是有专门的团队在做，一个人能不能做好这件事？

在这里我要先提出一个暴论：总体而言，大厂技术水平很烂。

大厂确实很有钱，每年都能招最好的学生，但是吧很多优秀学生进厂几年以后就泯然众人。
厂里面虽有很厉害的人或者小团队，但总体上说水平和效率都不高。
这是因为大厂里面有人负责干活，有人负责敲锣打鼓，有人负责生产锣鼓，有人负责拆解量化他们的工作，还有人负责工作汇报以证明这些人的工作都是有价值的；
有人想做点事但没有资源，掌握那些资源的人却在混日子。

要破除迷信，不用盲目崇拜大厂或者别的权威。

我现在做的这事情，比如性能优化，如果还在厂里面做，就需要先证明这个事情的价值，投入产出比是什么，对公司有什么好处，哦对了这些都要量化要拆解。
自己做的话，就不考虑那么多了，基础软件提升性能一定有价值，提升的幅度够大的话，量变就会成为质变，让不可能变成可能。

没有那么多杂事的干扰，一个人起步，是可以挑战大厂的团队的。另外项目也不会一直一个人做，会有合适的人在合适的时候加入进来。

## 当前的进展

一个分布式软件，首先要能够通信，所以先做rpc模块（dongting net），现在0.1-M4版本已经初步实现，两个jar包合计193KB，当前是做到了0依赖的：

* slf4j是可选依赖，运行时会自动探测，如果有就用，没有就用Java内置的JCL
* jctools用来实现MPSC内存队列，已经内联，包含在193KB之中，以后可能会自己写
* hppc用了IntObjectHashMap和LongObjectHashMap两个类，也已经内联，这玩意内联以后非常大，以后肯定会重写替代之

没有用netty；通信编码用的是proto buffer，自己解析。 （对了，我要讲个笑话，如果用pb官方的那个类库生成访问代码的话，一个最简单的pb message，编译以后的jar将近30KB。大型工程里面如果有成千的pb message，还都比较复杂，那么。。。）

去年在优化RocketMQ性能的时候，我发现有一部分性能问题和它的remoting（也就是RPC）模块有关。
为了改进这一点（以及其它一些问题），我基于netty，自己做了一个点对点的rpc程序demo（在我github仓库里面叫simple-rpc），但刚开始它的性能对比remoting没有决定性的提升。
然后我引入一个自动batch的机制，程序按TCP连接统计近期的数据，根据一个算法来推测这个连接打开批量是否划算，在运行过程中会不断调整。
这样大幅提升了性能，但是会引入最大1ms的延迟，这个simple-rpc最后也没能实装到RocketMQ上进行测试。

现在dongting net是全新开发，兼顾高吞吐和低延迟（我全都要）。
除了简单对比纯rpc模块的benchmark，这次还想看看综合效果，所以花了点时间集成到RocketMQ中测试，消息生产调用由rocketmq remoting换成了dongting net。
结果如下，下表中RT的单位是毫秒：

|                | TPS(Remoting) | TPS(dongting) | RT(Remoting) | RT(dongting) |
|----------------|---------------|---------------|--------------|--------------|
| 64线程128字节同步发送  | 103700        | 279000        | 0.616        | 0.23         |
| 单线程128字节异步发送   | 113000        | 650000        | 50           | 3.03         |
| 64线程4096字节同步发送 | 83000         | 175000        | 0.77         | 0.365        |
| 单线程4096字节异步发送  | 99000         | 197000        | 685          | 10.15        |

测试比较简单，就在单机进行，使用RocketMQ自带的benchmark程序，只生产（因为偷懒没改消费部分的代码）。
代码在这里，任何人都可以重测：https://github.com/areyouok/rocketmq/tree/dongting

可以看到，仅仅是更换rpc部分，整体性能就有了极大的提升，64线程128字节同步，替换后tps是原来的2.7倍，1单线程128字节异步，tps是原来的5.8倍，RT也大幅降低。

能做到这么大的提升得益于：

* 开发中尽量减少了线程切换的开销
* 在完全不影响RT的情况下，如果有多条数据等待读或者写，就执行批量读或写 
* 尽最大可能减少了复制的开销，基本上实现了零拷贝
* 尽量减少了buffer对象的创建，实现了一个单线程的对象池（这个目前很简陋，还有很大的改进空间）
* 代码以最高标准重新开发

全部完成以后肯定会有更好的效果。

## 接下来要做的事
dongting net还需要一些优化，之后就可以开始下一个阶段，也就是raft的实现，相信一定会超越etcd。

不好意思，这次又写了一篇没有任何干货的吹牛文。一些技术细节，无法在文中详述，我会另外撰文，应该就在这几天（之前一直没有写是因为还不是时候）。
近期至少有以下内容可写：

* 关于Java的内存模型（Java5、Java9），以及如何用来构建多线程安全的高性能内存队列，应该是多篇，可能是一个系列
* 关于网络高性能程序开发，这是我以前在厂里技术社区写的文章，当时写了两篇以后烂尾了，标题是“我的性能优化笔记”，我还记着呢，现在至少还需要再写两篇。
* 关于Java依赖治理、快速启动的一些思考，以前在厂里写过一些，会重新整理发出来
* 关于raft协议的介绍，这是我以前写过的，搬运一下

感谢关注，下次再见。

