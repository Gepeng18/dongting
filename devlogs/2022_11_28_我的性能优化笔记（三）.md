2021年10月，我在公司内网的技术论坛发表了关于网络性能优化的系列文章，写了两篇，然后烂尾了。现在有了更进一步的实践经验，这个系列终于可以续写了。让我们开始构建高性能的网络rpc。

## 背景
rpc是一切分布式应用的基础，dongting项目也是从底层rpc部分开始开发。

优化rpc的性能，最初的动机是发现在消息中间件的场景下，rpc至少是瓶颈之一，这导致硬件的能力没有被充分发挥。我做了一些工作，获得了一些数字上的改进，但是不够满意。我最初希望能够达到单机单进程百万TPS，然而仅仅是空跑rpc就做不到这一点，更不用说别的部分了。

实际上，百万TPS就是随口说说的目标，当时不知道极限能做到多少，所以要探测这个极限，越高越好。基于netty我自己肝了好些天做了一个简单的rpc程序，但在测试中，这个程序的表现比rocketmq的remoting强不了多少。

在简单的本地测试中，这个性能数字是单TCP连接10万TPS左右（视硬件能力和测试参数），只有目标值的1/10，增加客户端的数量和TCP连接的数量，不能成比例的提升吞吐，而且很快又会遇到CPU瓶颈。

在这个系列文章的前面的两篇中，甚至发现这个程序对比简单用传统InputStream/OutputStream的堵塞式IO程序也强不了太多。如果没有看过前两篇也没关系，因为前两篇文章大概就只讲了这么个悲惨的故事。

## RPC的实现方式

TCP协议是有背压流控的，比如下载一个文件的时候，网速快的时候下载就快，但接收方不会被淹没，网速慢的时候下载就慢，哪怕速度低于1KB/s也不会出错。既然如此，为什么RPC服务治理框架还有流控呢，它们之间有什么区别？

RPC框架的流控有很多上层的考虑，比如要快速失败等，这里先不讨论这些。TCP协议的背压流控是针对单个连接的，此外其实还有隐式的要求。

第一是它要求上层代码的IO操作是堵塞式的，比如既然网速慢，那发送方就会在发送的时候被堵塞（接收方也一样）。

第二它要求RPC服务方的业务处理是同步的，服务端接收到请求以后，只要在处理请求、写入响应完成之前不去执行读操作，请求发送方最终会被堵塞（窗口满），无法发送更多的请求。TCP的API接口没有直接规定这一点，不过对于上层的RPC来说，如果接到请求以后丢到其它线程异步去处理，那就不能堵塞请求发送方了，如果异步接收请求的速度比处理速度快（通常是这样），那么内存中异步处理的任务会堆积直到爆炸。

注意IO读写是否堵塞（同步），和业务逻辑是否同步处理，是互不相关的。服务端用一条基于BIO的连接，同样可以异步处理业务请求，这个时候，就可以用一条TCP连接同时跑多个请求（客户端在响应回来之前可以发下一个请求）。同时，就需要自己做流控以免被打爆。

这样，一个RPC框架有好几种实现方式。

第一种，使用BIO，同步处理请求。连接也可以复用，但是主调方必须读取了上一个请求的响应以后，才能发送下一个请求。在服务提供方，每个连接需要一个线程来处理，这个线程同时处理IO读写和业务逻辑。对于瓶颈在数据库操作的业务进程来说，4G内存开一两千个线程（也就是同时处理一两千个并发的请求）没问题的。这种方式最大的优点就是实现简单，一两千的并发请求能力对很多数据库型业务程序来说也不算小了。

第二种，使用BIO，异步处理请求。每个主调进程和服务提供进程只建立一条TCP连接，每个连接可以同时跑多个请求。在服务提供方，每个连接需要两个IO线程，一个负责读，一个负责写。和上一种方式相比，这种方式通常可以减少连接数和线程数。在“我的性能优化笔记（一）”中实现的就是这种。

第三种，使用非堵塞IO，异步处理请求。它的特点是可以用一个线程处理所有的IO读写请求（单线程容易有瓶颈，实践中通常用数个线程），IO线程数不随连接数的增长而增长，可以处理大量的连接。这种方式其实只有IO线程少这个优势，但是开发起来复杂了好多。

几乎所有的RPC框架都基于第三种方式开发，不过第二种方式已经可以满足绝大部分情况。连接数较不是很多的情况下，第二种方式的吞吐和第三种不会有太大区别，这是前面两篇文章的结论。

多说一句，第2、3种方式都是异步处理，需要框架自己实现流控。通常的处理手段是在堆积的请求数太多（或字节数太多）的时候失败。这对于前台应用是合适的，最终用户不会一直等着响应，超时了或者请求太多了就该失败掉。但对于后台任务类的应用来说，需要的是背压流控，流量太大应该堵塞请求发送方。大部分的RPC框架，反而丧失了这个TCP协议的基本能力（此外异步处理也丧失了TCP保持顺序的能力），业务开发者只好用自己的蹩脚方式来控制请求发送速度，量大了可能会失败，量小了不能充分利用服务提供方的资源，不能尽快完成批任务。这又是另一个复杂的议题，本文就不展开了。

## 第一次尝试
第一次尝试基于netty开发，最初TPS上不去的瓶颈就在于IO读写的系统调用过多，如果要提升吞吐，必须要减少系统调用的次数。批量读写可以做到这一点，但要求业务程序做改动，总是不太好。所以这个程序的主要思路都是在考虑，如何能够自动将IO写合并。

在客户端，有一个专门的线程负责写入，业务线程发送请求的时候，将请求写入一个内存队列，由这个专门的线程读出来然后写入netty channel，这个线程先探测当前的并发数（注意不是队列size），如果大于一个阈值，就执行批量发送。在服务端，采用了类似于Nagle的算法，会引入最多1ms的延迟。

这1ms的延迟在客户端同单线程步发送的时候，造成TPS无法超过1000。因此需要进行改进，光提供一个简单的开关是不行的，这个开关不能默认打开，用户也难以选择在什么情况下打开。所以默认提供了一个AUTO模式，程序按TCP连接统计近期的数据，根据一个算法来推测这个连接打开批量是否划算，在运行过程中会不断调整。AUTO模式在单线程同步发送时，延迟和DISABLE模式接近，而在异步发送时，吞吐和ENABLE模式接近。

这个程序在异步调用的情况下大幅提升了吞吐，但它写的很简单，很多地方有demo的性质（比如只能点对点，没有N对N的连接管理），要集成进rocketmq，还有很多工作要做，最终也没有实装。

* _注1：用netty的FlushConsolidationHandler来合并写，不好用_
* _注2：也可以根据netty的水位来决定是否合并写，这个就没试了_

这个程序的代码在这里，首页readme有详细的说明和测试数据：https://github.com/areyouok/simple-rpc

## dongting，再次出发
一年以后，dongting项目终于实现了完全没有额外等待的自动聚批。

因为项目是0依赖，没有了netty，对IO的控制更加自由了。相比于simple-rpc，dongting在以下方面做到了更高的性能，没有额外的代价。

第一，代码开发要求更高。这体现在方方面面，比如一次nanoTime()调用会有30ns左右的开销，dongting项目会尽量控制调用的次数，不随意调用。

第二，通过NIO的事件接口实现了自动聚批写的时候，完全不用等待。当socket是不可写状态的时，有数据要写就在队列里面堆着；状态变成可写时，就把这些数据合并写出去，只要不超过write buffer的大小，应写尽写，这样就实现了无等待的聚批。开发起来比基于netty还简单点。

第三，实现了尽可能少的内存复制。这个在请求或者响应数据包比较大的时候，会有一些提升。

作为对比，先看看netty会有那些可能的内存复制，如果你使用LengthFieldBasedFrameDecoder，它会在读到完整包（业务请求包，不是IP包）以后再给上层代码处理，如果没有读完整，它就先缓存着，等读完以后再给到上层。这个过程就涉及多余的内存占用或者复制。如果netty用一个4KB的buffer去读，而业务请求包有5KB，那么buffer读满了以后就要建一个新的大buffer，把原来的4KB复制过去才能继续读；或者一开始就用一个大buffer，但这很浪费内存，特别是io读比较慢的情况下（比如internet场景）框架层面需要较长时间持有这个buffer。因为粘包的原因，即便是大buffer读到最后，也会有半个包要面临拷贝的问题。netty不知道应该用多大的buffer合适，它其实是靠猜的，如果猜的不好就会有更多的复制开销。此外，netty会多一次从ByteBuffer到ByteBuf的复制开销。

一个请求进来，从字节流解码成上层业务程序需要的请求对象，这次复制是无法避免的。dongting的rpc协议解码器（protobuf）支持半包解析，一次io read完成后，即便业务请求包没有读完整，也可以开始解析，大部分中间状态直接暂存在最终对象上，等于是直接从io buffer（通常是堆外ByteBuffer）解码到最终业务对象，完全没有多余的复制。

_注：rpc协议包的body字段通常是上层业务的包，这里等于包装了两层，如果上层业务支持半包解析（比如又是个支持半包解析的protobuf），可以免复制，否则协议解码器会复制一个ByteBuffer给上层业务解码。_

第四，由于前面支持了半包解析，每次io读取、解码调用完成（整个业务请求包可能没有解码完成，但本次io读到的数据已经解码完成）以后，io buffer里面没有残留的数据，这个io buffer可以用于下一次读取。io线程总共没几个，每个线程都可以用一个专用大buffer去读，这等于是提升了批量读的效率。

最终，dongting在简单的单机测试中同步异步各方面都吊打simple-rpc。最大的数字是128字节单线程异步可以到200万+，而且这是在单连接的情况下测得的，CPU使用率极低，潜力还很大。

## 结束语
RPC优化的文章暂时告一段落，也许以后能较好的实现异步调用的N对N流控（TCP的窗口协商只是一对一）和保序会再写吧，但现在也不知能不能做到，所以这个文章也可能没有后续了。

dongting现在只初步实现了low level rpc，本身还不能实现任何业务功能，剩下没有完善的部分，就在接下来的具体应用中完善。

近期不会再写新文，最多是整理以前写过的文档，该写程序了，希望下次再报告进展的时候，raft实现应该已经开发完成，到时候我们把etcd拉出来打一顿。

