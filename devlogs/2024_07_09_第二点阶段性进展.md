没想到，距离上次写“第一点阶段性进展”已经过了19个月了，距离我上次在公众号写文章都过去快1年了。现在Dongting终于完成了RAFT框架的开发工作，甚至还没有完成测试，不过总算可以进行一点玩具级别的demo和性能演示了。

3年前我曾经在快手内网发表文章，提到“想做一个简单的高性能点对点的RPC框架玩，它的性能应该至少要达到100万TPS，而且限定单实例、单TCP连接、实时调用、实时返回”，当时有人在评论认为这不太可能做到，现在，我可以说已经实现了目标，而且还是带磁盘IO的线性一致性系统哦。

去年9月，我把刚做完单元测试的store模块，对接到RPC进行测试，结果不太满意，然后就开始做性能优化。没想到一做就是将近10个月。几乎把所有能想到的优化点都优化了一遍，核心链路上所有的东西全部改成异步的，太苦逼了。

我甚至自己做了一套协程（虚拟线程）调度系统，是不是必须这样我也讲不清了，关于它的细节下次写文章再说吧。我也不止一次地想，如果Java 21普及了，我做的这玩意还有用吗？不过至少现在，它保证了dongting server在Java 11下可用，而且性能也比Java 21的虚拟线程要好（因为它是一个侵入式的无栈实现）。

现在Dongting的功能就是一个RAFT框架，对标etcd。从上层功能来看，Dongting当前只实现了最简单demo级别K/V状态机，肯定是没有etcd丰富的，不过上层的功能非常容易开发，而Dongting的底层功能远比etcd强大，因为Dongting支持multi-raft，可以做数据分片动态增删（这一点类似TiKV），功能的上限是远超etcd的。从性能来讲，之前我提到项目的目标是2～10倍TPS，其实2倍是兜底的保守说法，我真正希望的是10倍TPS，那么Dongting的性能有没有etcd的10倍呢？

我找到了etcd的官方说明：https://etcd.io/docs/v3.5/op-guide/performance/

etcd数据如下：4台云服务器，100,000个key，256字节数据，1000个client，100个连接，写入50,104TPS，20ms延迟。

我现在不上班只有吃饭的钱，所以没有4个云服务器，只在自己的PC机上做了测试，大致对比一下。我的PC配置如下（不是多好的机器，大概是4年前的主流配置了）：

* AMD 5600X 6-core CPU, 12 threads
* 32GB DDR4 3600MHz RAM
* 1TB PCI-E 3.0 SSD，估计这里是性能瓶颈
* Windows 11、JDK 17 with -XX:+UseZGC -Xmx4G -Xms4G

我把client和server都运行在一个进程里，使用127.0.0.1进行TCP通信。etcd测试的client goroutine是同步的，因此非常弱智地需要1000个client才把server压到最大吞吐。Dongting的client是异步的，只要1个client和1个TCP连接就能达到最大性能，为了公平我也开了100个client（含100个TCP连接，200个线程，这样性能略下降），200个系统线程对1000个goroutine我肯定是吃亏了，4个节点运行在一个PC上我也大大吃亏，另一方面走127.0.0.1通信我应该占了点便宜，IO性能不确定是我的PC好还是etcd的压测集群更好。

最后结果是Dongting写入230,889TPS，延迟9ms，吞吐只有etcd的4倍多，但我对于实现10倍吞吐还是有信心的，3个server和1个client挤在一个机器上性能实在很受影响，把server改为1个，tps就变成了50多万，就这server和client还挤在一个机器上呢。

再简单看了下TiKV官方的benchmark，感觉和etcd差不多甚至还差点？跑个测试单机40核CPU是什么意思？不管了，在50万看来5万和8万有什么区别呢？

Dongting默认是同步刷盘，每个写请求，都要写入完成并且调用fsync（即FileChannel.force()）以后，follower才会向leader报告写入完成，leader要完成自己的fsync才会commit并返回结果给client。要知道fsync调用一下就要1ms，一个进程同步刷盘的情况下要同时做到高TPS、低延迟并且打满磁盘IO是非常困难的（不信你可以写个程序试试），但Dongting做到了。这背后的苦力活特别多，比如数据来了就必须马上写（要满足低延迟要求，不能等数据满1000条或者1秒钟），写完马上就调用fsync刷盘，这意味虽然是append日志也得并发写，不会等前一个操作完成才写下一个，同时又要尽可能聚批以提升吞吐，并发写入不一定按顺序返回，但线性一致性的系统得按顺序梳理。

在1个server的情况下，进一步修改刷盘为异步模式，并将client改为1个，此时TPS达115万，延迟1.7ms。

异步刷盘有什么用？在Raft3副本的情况下，只要不同时掉电，数据就不会丢。RocketMQ默认也是异步刷盘（同时默认异步复制）。正巧我在“第一点阶段性进展”中做过RocketMQ的压测，也是在我的这台PC上，而且Dongting以后要实现MQ功能的，raft log其实就可以作为commit log，不会有性能损失，Dongting MQ和RAFT的性能是基本一致的，这样我们有了一个同硬件配置下的对比对象。

128字节数据情况下，不管怎么弄，RocketMQ在这台PC上的生产TPS极限就是11万左右（以前曾经在公司物理服务器上测试到25万左右的单机生产TPS）。RocketMQ大致有2个瓶颈，在“第一点阶段性进展”中将RocketMQ remoting更换为Dongting Remoting后达到了65万TPS，另一个瓶颈和mmap有关。Dongting在128字节（之前256）并且简化状态机（RocketMQ没有状态机，因此将将KEYS设置为1）的情况下达到了178万TPS，延迟1.1ms，吞吐16倍于RocketMQ。

以上这些，都是在单一分片的情况下做到的。如果状态机复杂或者生产环境工况复杂导致单分片瓶颈，通过multi raft增加分片能进一步更加充分使用CPU和其它硬件资源。

Dongting已经实现了计划中的零依赖，这意味着：

* 不依赖也不传递第三方jar包。Slf4j是个可选依赖，如果没有走jdk logging，日志量不大，不会有什么问题。
* 对JDK没有特别的太多特别的要求。只有两个jar，合计小于1MB，client要求Java 8，server要求Java 11。
* 对硬件没有特别的依赖，不会要求使用40核的服务器做压测，我觉得4核就够了，更不会要求使用什么RDMA或者傲腾之类的东西来达到一个非常好看的数字上限，甚至使用机械硬盘也有不错的性能。或者要不咱们上树莓派比比？
* 对云服务没有特别的依赖，所有的事情自己解决，不会把复杂性丢给特定厂商的云服务然后绑定它。
* 对于内核参数没有特别的依赖（你可能没有权限调整内核参数）

接下来的事情，首先是做稳定，RAFT框架功能和性能总算是做完了，但一些流程（比如成员变更和增减group）甚至都还没跑过，至少把bug修了，把测试补完，从玩具级别到准生产可用级别吧。DtKV的功能要完善一些做成一个config server，总不能只有put/get。

此外，要写一篇文章介绍下Dongting fiber，项目现在没有文档，关于RAFT相关的一些实现要点，也要写文章记录下，就当是文档吧。