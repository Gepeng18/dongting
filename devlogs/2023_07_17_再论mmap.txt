在编写高性能存储程序的时候，有一项称为mmap（在Java里面就是MappedByteBuffer）的内存管理技术，它将文件或其它对象映射到进程的地址空间中，允许文件数据被视作内存进行访问，可以大大提高文件读取和写入的效率。
具体来说，Java中可通过FileChannel的mmap方法将一个文件映射成MappedByteBuffer，然后像操作ByteBuffer一样读写它，如果读取的内容不在page cache中，操作系统会安排加载，写入时只写内存，操作系统会安排异步刷盘，能减少IO次数，用户程序也可以调用force方法强制刷盘。这意味着，如果page cache命中，读操作和读内存效率大致是一样的，绝大部分情况下写操作也和写内存效率相当。
dongting raft实现的日志部分也需要读写文件，项目定义了一个名为RaftLog的接口，允许日志模块有多种不同的实现。我曾经和使用mmap的中间件打过长时间的交道，dongting当前的默认RaftLog实现，却没有使用mmap，本文分析一下其中的取舍。

我们先来讨论mmap的各种优点。首先就是很多人熟知的，使用mmap比用普通的方式少一次内存拷贝。如果用户程序自己使用一个buffer去读文件，操作系统实际上需要先读入系统buffer，然后再复制到用户程序的buffer（从系统内存到用户内存）；使用mmap的话，等于是直接在使用系统buffer，就少了这次拷贝。近年来SSD的访问速度突飞猛进，顺序读（或写）SSD的吞吐和内存拷贝相比，差距已经不到10倍了，在优化做的特别好的场景下，少一次内存拷贝是很有意义的。当然大部分程序瓶颈不在这里，可能感知不出这个差异。有人会把这个称作零拷贝，其实是不严谨的，更准确的说，应该说是减少一次拷贝，现实中的程序很难做到一次也不拷贝。

第二，mmap可以充分利用空闲的内存。它和操作系统page cache是对齐的，在linux下，操作系统认为空闲的内存不用白不用，会用它们来缓存加载过的文件数据，比如系统内存一共128GB，操作系统和用户进程只用10GB，那剩下的100+GB都可以用作缓存。这比用户在自己的进程内存中做缓存有几个优势：操作系统管理的page cache是弹性的，可多可少，需要内存的时候可以释放一些，而用户进程如果上来就申请100+GB内存做缓存，其它进程向操作系统申请内存的时候可能会失败，这个后果是很严重的；用户进程不能高效管理大内存，比如大内存虚拟机可能会面临gc缓慢的问题；操作系统的内存管理算法通常比用户自己能实现的要好；如果用户进程崩溃，重启以后它之前管理的缓存数据就没有了，甚至写操作的数据没有刷盘，而page cache不会因为用户进程崩溃而失效，除非操作系统崩溃或断电，这个概率比用户进程崩溃低多了。
mmap还有很重要的一个特性，它可以映射的数据总量（虚拟内存）可以远超物理内存大小，比如128GB的物理内存，映射总量10个TB的数据是没有一点问题的，数据真正要读到的时候才会执行IO操作加载，已经写入的数据只有当内存不足的时候才会异步刷盘释放内存，由操作系统来管理page cache的加载和淘汰，特别省事、高效。

第三点，mmap可以让多个进程共享数据。我通常用不上这一点，这里就不深入了。

最后这一点可能很少有别的地方提到，在实际工程中，mmap比普通方式减少的不止是一次拷贝。文件不是特别大（下文会提到具体限制）的场景，整个文件可以mmap映射到一个buffer当中，读写的时候不存在边界问题。普通IO，通常会用一个固定的，不是特别大的buffer（比如说128KB），那么读取数据的时候，就可能会遇到最后读了半条记录的问题（比如一次读入128KB数据，包含2条完整的记录和半条不完整的记录），如果要读完整条记录再做解析，就免不了要把这半条记录数据挪动（复制）一下。对于写来说，普通的IO更容易遇到buffer剩余空间不够写一条记录的问题，需要更麻烦的处理。
如果说上面提到的复制半条记录对性能影响不大（主要是开发麻烦），那么接下来要提到的这一点就实实在在多了几次内存拷贝。一条数据记录可能分为多层，就像TCP报文那样，物理层有header，IP层有header，TCP层有header，最后才是用户数据。一般来说，解码的时候也需要层层解开（涉及复制）。使用mmap映射整个文件的话，可以用slice创建新的buffer，和原先的buffer共享底层的数据，免去拷贝；普通IO由于读取用的buffer是复用的，里面的数据内容会被下次读取覆盖，所以不能用slice。

有了这么多好处，当然也会有缺点。我们先来看个小缺点，Java里面用32位有符号int来表示buffer的长度，能支持的最大buffer，也就是int的最大值是2G-1字节，mmap最多也只能映射这么大的文件，超过这个大小的文件，需要多次映射到多个buffer里面去。如果要求文件大小是2的幂以便进行某些处理，那最大就只能支持1G的文件。这个小限制通常倒是不会造成什么影响。

第二点，mmap的buffer的读写通常等同于内存读（读命中的情况下）写，但这完全由操作系统控制，开发人员自己毫无办法，一个buffer写操作，以为是内存操作，结果因为操作系统的调度原因就堵塞了。如果我们想做一个全异步的程序，用一个线程来干所有的事情，这个堵塞会对性能造成严重的影响。现代SSD随机读写延迟只有几十微秒，如果只是堵塞几十微秒去读写一下数据，似乎也还能接受，遗憾的是，如果映射出来的虚拟内存远超物理内存，操作系统的内存管理负担也会非常大，这个卡顿可能会持续非常久，它不仅仅是等待IO，更有一系列CPU密集的工作需要完成，当一个写操作过来的时候，虽然用户没有要求立即flush，但操作系统可能发现这个buffer对应的数据块还不在内存中，为了初始化这个数据块，操作系统要在内存里面腾出地方来，将其它的page cache先释放一些，如果对应的page是脏的（有过更新但没有flush），需要先flush，如果新初始化的数据块在文件中已经存在，还要先读进来。linux下的kswapd进程负责page cache的回收（它还负责别的事情），这个进程是单线程的，每个物理CPU（不是core）会对应这样一个进程，它会成为瓶颈。以写日志文件为例，用mmap映射大量日志文件，并不断高速append数据，新写入的数据不断追加，就不断要求淘汰老的page，这个时候我们可能会看到kswapd进程的CPU占用偶尔达到或接近100%（单个核心占用，它只有一个线程），此时卡顿时长偶尔会达到惊人的数值。
这些是我从线上实践中得到的经验，从理论上还能得出一些推论。比如我猜测系统内存越大，这个问题就越严重，因为kswapd负担就更大，同时，如果划分成多个虚拟机，有助于缓解这个问题，因为各个虚拟机都有自己的kswapd进程，并且都只有一部分内存。这些猜测就我没有实测去验证了。

第三点，mmap可能会造成容器隔离性不足。容器技术是不能隔离page cache的，如果某个容器大量mmap造成了上面提到的第二点问题，那么其它容器分配内存的时候也会受到影响。如果是通过虚拟机隔离则不会受到这个影响。

最后一点，在极端情况下，使用mmap的可控性更差。比如上面第二点问题，通过调整一些内核参数可以优化这个性能（这个调优是一件非常恼火的工作），新版本的Linux内核表现远远好于3.X的版本。mmap的文件多了可能还需要调整其它内核参数比如vm.max_map_count。但开发人员很多时候没有权限去调整内核参数，也无权选择操作系统。

分析了这么多利弊之后，dongting最后还是先（暂时）选择了普通的IO方式，没有用mmap。这里的考虑主要是希望程序在各种环境下能有稳定的性能表现，而不要有太多前置条件。也可能是以前和mmap打交道太多了，现在想换一种方式。项目里面通过半包编解码消除了不少拷贝，能缩小一些与mmap的性能差距。

其实我原先一直想做两个RaftLog实现的，然后把这两个进行一下性能对比。不过被现实毒打以后，还是决定先只做一个比较好，毕竟没有那么多时间，这可能是当前没有做mmap实现的主要原因T_T。